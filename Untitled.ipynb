{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55fd5d2d-5030-46e1-ae35-743ec9dc0335",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-d87bfcbc1b0a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPorterStemmer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mstemer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mPorterStemmer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "stemer=PorterStemmer()\n",
    "\n",
    "import numpy as np\n",
    "import tflearn as tfl\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "class chatbot:\n",
    "\n",
    "    def fix_data():\n",
    "            \n",
    "        with open(\"intents.json\") as file:\n",
    "            data = json.load(file)\n",
    "\n",
    "        #print(data[\"intents\"]) \n",
    "        words = []\n",
    "        labels = []\n",
    "        docs_x = []\n",
    "        docs_y = []\n",
    "        try:\n",
    "            with open('data.pickle','rb') as f:\n",
    "                words,labels,training.output=pickle.load(f)\n",
    "\n",
    "        except:\n",
    "            for intent in data[\"intents\"]:\n",
    "                for pattern in intent[\"patterns\"]:\n",
    "                    wrds = nltk.word_tokenize(pattern)\n",
    "                    words.extend(wrds)\n",
    "                    docs_x.append(wrds)\n",
    "                    docs_y.append(intent[\"tag\"])\n",
    "\n",
    "                if intent[\"tag\"] not in labels:\n",
    "                    labels.append(intent[\"tag\"])\n",
    "\n",
    "            words = [stemer.stem(w.lower()) for w in words if w != \"?\"]\n",
    "            words = sorted(list(set(words)))\n",
    "\n",
    "            labels = sorted(labels)\n",
    "\n",
    "            training = []\n",
    "            output = []\n",
    "            out_empty = [0 for _ in range(len(labels))]\n",
    "            for x,doc in enumerate(docs_x):\n",
    "                bag = []\n",
    "                wrds = [stemer.stem(x) for x in doc]\n",
    "                for w in words:\n",
    "                    if w in wrds:\n",
    "                        bag.append(1)\n",
    "                    else :\n",
    "                        bag.append(0)\n",
    "                output_row = out_empty[:]\n",
    "                output_row[labels.index(docs_y[x])]=1\n",
    "\n",
    "                training.append(bag)\n",
    "                output.append(output_row)\n",
    "\n",
    "            training= np.array(training)\n",
    "            output = np.array(output)\n",
    "\n",
    "            with open('data.pickle','wb') as f:\n",
    "                pickle.dump((words,labels,training,output),f)\n",
    "        try:\n",
    "            # Load a model\n",
    "            tfl.model.load(\"model.tflearn\")\n",
    "        except: \n",
    "                #tf.reset_default_graph()\n",
    "            tf.compat.v1.reset_default_graph()\n",
    "\n",
    "            net = tfl.input_data(shape=[None,len(training[0])])\n",
    "            net = tfl.fully_connected(net,8)\n",
    "            net = tfl.fully_connected(net,8)\n",
    "            net = tfl.fully_connected(net,len(output[0]),activation=\"softmax\")\n",
    "            net = tfl.regression(net)\n",
    "\n",
    "            model = tfl.DNN(net)\n",
    "\n",
    "            model.fit(training,output,n_epoch=99,batch_size=8,show_metric=False)\n",
    "            # Manually save model\n",
    "            model.save(\"model.tflearn\")\n",
    "        return model,labels,data,words\n",
    "\n",
    "def bag_of_words(s,words):\n",
    "    bag = [0 for _ in range(len(words))]\n",
    "    s_words = nltk.word_tokenize(s)\n",
    "    s_words = [stemer.stem(word.lower()) for word in s_words]\n",
    "\n",
    "    for se in s_words:\n",
    "        for i,w in enumerate(words):\n",
    "            if w == se:\n",
    "                bag[i]=1\n",
    "    return np.array(bag)\n",
    "\n",
    "def chat():\n",
    "    quit_ = set([\"q\",\"x\",\"c\",\"quit\",\"exit\",\"exit()\"])\n",
    "    model,labels,data,words = chatbot.fix_data()\n",
    "    print(\"Welcome!, type your query\")\n",
    "    while True:\n",
    "        in_text = input(\"You :\" )\n",
    "        if in_text.lower() in quit_:\n",
    "            break\n",
    "        result = model.predict([bag_of_words(in_text,words)])\n",
    "        result_idx = np.argmax(result)\n",
    "        tag = labels[result_idx]\n",
    "        for tag_ in data[\"intents\"]:\n",
    "            if(tag==tag_['tag']):\n",
    "                response = tag_['responses']\n",
    "        print(random.choice(response))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65120f02-f2ad-4cca-9945-25bed0244605",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))': /simple/nltk/\n",
      "WARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))': /simple/nltk/\n",
      "WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))': /simple/nltk/\n",
      "WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))': /simple/nltk/\n",
      "WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))': /simple/nltk/\n",
      "ERROR: Could not find a version that satisfies the requirement nltk (from versions: none)\n",
      "ERROR: No matching distribution found for nltk\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "884c323c-c378-48e5-bd30-7febd5ed7959",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Travelling Ways\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c605ad2b-f24e-4321-80e4-74ffab455b2a",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for array: array is 0-dimensional, but 1 were indexed",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-bae6ac0ea40d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mgrid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGrid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrid_init\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-18-44df833f5370>\u001b[0m in \u001b[0;36mgrid_init\u001b[1;34m(r, c)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mgrid_init\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mc\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[0mgrid\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mgrid\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-18-44df833f5370>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mgrid_init\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mc\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[0mgrid\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mgrid\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-18-44df833f5370>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mgrid_init\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mc\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[0mgrid\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mgrid\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for array: array is 0-dimensional, but 1 were indexed"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    r = 2\n",
    "    c = 2\n",
    "    grid = Grid.grid_init(r,c)\n",
    "    print(type(grid))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "931d5ce7-85a2-4a46-adc7-91fd34bfe5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Grid:\n",
    "    def grid_init(r,c ):\n",
    "        grid  = np.array(4)\n",
    "        return [ [grid[i][j] for i in range(r)] for j in range(c)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d29c52-7937-4f9a-a1d0-98f3b42bfc34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
